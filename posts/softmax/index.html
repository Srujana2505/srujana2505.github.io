<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Softmax | Sicilian</title><meta name=keywords content="softmax,backprop,code"><meta name=description content="Softmax takes an N-dimensional vector as input and outputs an N-dimensional vector of probabilities which sums up to 1. It is a generalised version of logistic regression but with multi classes instead of two classes hence it can be used for multi-class classification.
Softmax equation: $$A = \frac{e^z}{\Sigma e^z}$$
The ground truth value for a softmax function is a one-hot encoded vector.
Cross-entropy loss Cross entropy loss can be derived from the below expression."><meta name=author content="Sicilian"><link rel=canonical href=https://srujana2505.github.io/posts/softmax/><link crossorigin=anonymous href=/assets/css/stylesheet.3613efbd0b1772781e8f49935e973cae632a7f61471c05b17be155505ccf87b5.css integrity="sha256-NhPvvQsXcngej0mTXpc8rmMqf2FHHAWxe+FVUFzPh7U=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://srujana2505.github.io/assets/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://srujana2505.github.io/assets/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://srujana2505.github.io/assets/favicon-32x32.png><link rel=apple-touch-icon href=https://srujana2505.github.io/assets/apple-touch-icon.png><link rel=mask-icon href=https://srujana2505.github.io/assets/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.css integrity=sha384-3UiQGuEI4TTMaFmGIZumfRPtfKQ3trwQE2JgosJxCnGmQpL/lJdjpcHkaaFwHlcI crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/katex.min.js integrity=sha384-G0zcxDFp5LWZtDuRMnBkk3EphCK1lhEf4UEyEM693ka574TZGwo4IWwS6QLzM/2t crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.7/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}]})})</script><meta property="og:title" content="Softmax"><meta property="og:description" content="Softmax takes an N-dimensional vector as input and outputs an N-dimensional vector of probabilities which sums up to 1. It is a generalised version of logistic regression but with multi classes instead of two classes hence it can be used for multi-class classification.
Softmax equation: $$A = \frac{e^z}{\Sigma e^z}$$
The ground truth value for a softmax function is a one-hot encoded vector.
Cross-entropy loss Cross entropy loss can be derived from the below expression."><meta property="og:type" content="article"><meta property="og:url" content="https://srujana2505.github.io/posts/softmax/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-06-09T21:57:31+05:30"><meta property="article:modified_time" content="2023-06-09T21:57:31+05:30"><meta property="og:site_name" content="Sicilian"><meta name=twitter:card content="summary"><meta name=twitter:title content="Softmax"><meta name=twitter:description content="Softmax takes an N-dimensional vector as input and outputs an N-dimensional vector of probabilities which sums up to 1. It is a generalised version of logistic regression but with multi classes instead of two classes hence it can be used for multi-class classification.
Softmax equation: $$A = \frac{e^z}{\Sigma e^z}$$
The ground truth value for a softmax function is a one-hot encoded vector.
Cross-entropy loss Cross entropy loss can be derived from the below expression."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://srujana2505.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Softmax","item":"https://srujana2505.github.io/posts/softmax/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Softmax","name":"Softmax","description":"Softmax takes an N-dimensional vector as input and outputs an N-dimensional vector of probabilities which sums up to 1. It is a generalised version of logistic regression but with multi classes instead of two classes hence it can be used for multi-class classification.\nSoftmax equation: $$A = \\frac{e^z}{\\Sigma e^z}$$\nThe ground truth value for a softmax function is a one-hot encoded vector.\nCross-entropy loss Cross entropy loss can be derived from the below expression.","keywords":["softmax","backprop","code"],"articleBody":"Softmax takes an N-dimensional vector as input and outputs an N-dimensional vector of probabilities which sums up to 1. It is a generalised version of logistic regression but with multi classes instead of two classes hence it can be used for multi-class classification.\nSoftmax equation: $$A = \\frac{e^z}{\\Sigma e^z}$$\nThe ground truth value for a softmax function is a one-hot encoded vector.\nCross-entropy loss Cross entropy loss can be derived from the below expression.\n$$ P(\\frac{y}{x}) = \\hat{y}^y(1-\\hat{y})^{1-y}$$\nWe apply log on both sides\n$$\\log P(\\frac{y}{x}) = \\log \\hat{y}^y + \\underbrace{\\log (1-\\hat{y})}^{1-y}$$\nWe calculate the loss only when y = 1(ground truth value). So when y = 1 the second term in the above equations becomes zero and we are left with the first term. And as we want to define a loss function we would like to minimize it and hence we can place a negative symbol before the equation.\n$$C.E.loss = -y\\log \\hat{y}$$\nBackprop Since softmax is a vector function we canâ€™t just calculate the derivative of it. The derivatives will be another vector and we can use a Jacobian matrix for it.\nThe above network is considered for backprop. The output layer is a softmax layer\nOur final goal for back-propagation is to update weights and biases so we can minimize the loss. And to update the parameters with gradient descent we need the derivatives dL/dW and dL/db(Where L is the loss function, W, b are the weights and biases of the network). We calculate the two derivatives using the chain rule.\nL -\u003e cross entropy loss W -\u003e weights of the network b -\u003e biases of the network A -\u003e softmax activation function or $\\hat{y}$ z -\u003e $wx + b$ Chain rule for dL/dw and dL/db:\n$$\\frac{dL}{dW} = \\frac{dL}{dA}.\\frac{dA}{dz}.\\frac{dz}{dw}$$\n$$\\frac{dL}{db} = \\frac{dL}{dA}.\\frac{dA}{dz}.\\frac{dz}{db}$$\nSo we need to calculate $\\frac{dL}{dA}$, $\\frac{dA}{dz}$, $\\frac{dz}{dw}$, $\\frac{dz}{db}$.\nCalculating $\\LARGE\\frac{dL}{dA}$: $$L = -\\Sigma {y_i}\\log{A_i}$$ $$\\frac{dL}{dA} = -\\Sigma\\frac{y_i}{A_i}$$\nCalculating $\\LARGE\\frac{dA}{dz}$:\nConsider the activation of ith neuron. Then we have two cases where we consider the ith dz or jth dz.\nwhen i = j: $$\\frac{A_i}{z_i} = \\frac{e^{z_i}}{\\Sigma e^{z_i}}$$\n$$\\frac{dA_i}{dz_i} = \\frac{e^{z_i}(\\Sigma e^{z_i})-(e^{z_i})^2}{(\\Sigma e^{z_i})^2}$$\n$$\\frac{dA_i}{dz_i} = A_i(1-A_i)$$\nwhen i $\\not =$ j: $$\\frac{dA_i}{dz_j} = \\frac{0-e^{z_i}e^{z_j}}{(\\Sigma e^z)^2}$$\n$$\\frac{dA_i}{dz_j} = -A_iA_j$$\nCalculating $\\LARGE{\\frac{dz}{dw}, \\frac{dz}{db}}$: $$z = wx + b$$ $$\\frac{dz}{dw} = x$$ $$\\frac{dz}{db} = 1$$\nCalculating $\\LARGE\\frac{dL}{dz}$: $$\\frac{dL}{dz} = \\frac{dL}{dA}.\\frac{dA}{dz}$$ $$\\frac{dL}{dz} = - \\Sigma\\frac{y_i}{A_i}.\\frac{dA}{dz}$$ $$\\frac{dL}{dz}= -\\frac{y_i}{A_i}.\\frac{dA_i}{dz_i}- \\Sigma\\frac{y_j}{A_j}.\\frac{dA_i}{dz_j}$$\nsubstituting the above da/dz values: $$\\frac{dL}{dz} = -\\frac{y_i}{A_i}.{A_i}(1-{A_i})- \\Sigma\\frac{y_j}{A_j}.(-{A_i}{A_j})$$ $$= - {y_i} + {y_i}{A_i} + \\Sigma{y_j}{A_i}$$ $$= - {y_i} + {y_i}{A_i} + {A_i}\\Sigma{y_j}$$ $$= - {y_i} + A_i\\Sigma y$$ $$\\frac{dL}{dz}= A_i - y_i$$\nCalculating $\\LARGE\\frac{dL}{dw}$: $$\\frac{dL}{dw} = \\frac{dL}{dz}.\\frac{dz}{dw}$$ $$\\frac{dL}{dw} = X(A_i - y_i)$$\nCalculating $\\LARGE\\frac{dL}{db}$: $$\\frac{dL}{db} = \\frac{dL}{dz}.\\frac{dz}{db}$$ $$\\frac{dL}{db} = A_i - y_i$$\nCode import numpy as np def softmax(z): a = np.exp(z) a = a/np.sum(a, axis = 0, keepdims=True) return a def init_parameters(): w = np.random.randn(1, 5)*10 b = np.zeros(5) return w, b def cross_entropy_loss(A, y): loss = -np.mean(y*np.log(A)) return loss def forward(w, X, b): z = np.dot(X, w.T) + b A = softmax(z) return A def backprop(A, X, y, w, b, alpha): dz = A - y print('dz: ', dz) print(X) dw = dz*X db = A - y print('dw: ', dw) w = w - alpha*dw b = b - alpha*db return w, b alpha = 0.7 X = np.array([1, 2, 3, 4, 5]) y = np.array([0, 0, 0, 0, 1]) w, b = init_parameters() print('initial w, b: ', w, b) A = forward(w, X, b) print('A: ', A) w, b = backprop(A, X, y, w, b, alpha) print('after backprop: ', w, b) ","wordCount":"586","inLanguage":"en","datePublished":"2023-06-09T21:57:31+05:30","dateModified":"2023-06-09T21:57:31+05:30","author":{"@type":"Person","name":"Sicilian"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://srujana2505.github.io/posts/softmax/"},"publisher":{"@type":"Organization","name":"Sicilian","logo":{"@type":"ImageObject","url":"https://srujana2505.github.io/assets/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://srujana2505.github.io/ accesskey=h title="Sicilian (Alt + H)">Sicilian</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://srujana2505.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://srujana2505.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://srujana2505.github.io/tags/ title=Tags><span>Tags</span></a></li><li><a href=https://srujana2505.github.io/archives/ title=Archives><span>Archives</span></a></li><li><a href=https://srujana2505.github.io/search/ title="Search (Alt + /)" accesskey=/><span>Search</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://srujana2505.github.io/>Home</a>&nbsp;Â»&nbsp;<a href=https://srujana2505.github.io/posts/>Posts</a></div><h1 class=post-title>Softmax</h1><div class=post-meta><span title='2023-06-09 21:57:31 +0530 +0530'>June 9, 2023</span>&nbsp;Â·&nbsp;3 min&nbsp;Â·&nbsp;Sicilian</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#cross-entropy-loss>Cross-entropy loss</a></li><li><a href=#backprop>Backprop</a></li><li><a href=#code>Code</a></li></ul></nav></div></details></div><div class=post-content><p>Softmax takes an N-dimensional vector as input and outputs an N-dimensional vector of probabilities which sums up to 1. It is a generalised version of logistic regression but with multi classes instead of two classes hence it can be used for multi-class classification.</p><p>Softmax equation:
$$A = \frac{e^z}{\Sigma e^z}$$</p><p>The ground truth value for a softmax function is a one-hot encoded vector.</p><h2 id=cross-entropy-loss>Cross-entropy loss<a hidden class=anchor aria-hidden=true href=#cross-entropy-loss>#</a></h2><p>Cross entropy loss can be derived from the below expression.</p><p>$$ P(\frac{y}{x}) = \hat{y}^y(1-\hat{y})^{1-y}$$</p><p>We apply log on both sides</p><p>$$\log P(\frac{y}{x}) = \log \hat{y}^y + \underbrace{\log (1-\hat{y})}^{1-y}$$</p><p>We calculate the loss only when y = 1(ground truth value). So when y = 1 the second term in the above equations becomes zero and we are left with the first term. And as we want to define a loss function we would like to minimize it and hence we can place a negative symbol before the equation.</p><p>$$C.E.loss = -y\log \hat{y}$$</p><h2 id=backprop>Backprop<a hidden class=anchor aria-hidden=true href=#backprop>#</a></h2><p>Since softmax is a vector function we can&rsquo;t just calculate the derivative of it. The derivatives will be another vector and we can use a Jacobian matrix for it.</p><figure><img loading=lazy src=/images/nn.png alt="The above network is considered for backprop. The output layer is a softmax layer"><figcaption><p>The above network is considered for backprop. The output layer is a softmax layer</p></figcaption></figure><p>Our final goal for back-propagation is to update weights and biases so we can minimize the loss. And to update the parameters with gradient descent we need the derivatives <em>dL/dW</em> and <em>dL/db</em>(Where L is the loss function, W, b are the weights and biases of the network). We calculate the two derivatives using the chain rule.</p><ul><li>L -> cross entropy loss</li><li>W -> weights of the network</li><li>b -> biases of the network</li><li>A -> softmax activation function or $\hat{y}$</li><li>z -> $wx + b$</li></ul><figure><img loading=lazy src=/images/flow.png></figure><p>Chain rule for dL/dw and dL/db:</p><p>$$\frac{dL}{dW} = \frac{dL}{dA}.\frac{dA}{dz}.\frac{dz}{dw}$$</p><p>$$\frac{dL}{db} = \frac{dL}{dA}.\frac{dA}{dz}.\frac{dz}{db}$$</p><p>So we need to calculate $\frac{dL}{dA}$, $\frac{dA}{dz}$, $\frac{dz}{dw}$, $\frac{dz}{db}$.</p><ul><li><strong>Calculating $\LARGE\frac{dL}{dA}$:</strong>
$$L = -\Sigma {y_i}\log{A_i}$$</li></ul><p>$$\frac{dL}{dA} = -\Sigma\frac{y_i}{A_i}$$</p><ul><li><p><strong>Calculating $\LARGE\frac{dA}{dz}$:</strong></p><ul><li><p>Consider the activation of <em>ith</em> neuron. Then we have two cases where we consider the <em>ith</em> dz or <em>jth</em> dz.</p></li><li><p>when i = j:
$$\frac{A_i}{z_i} = \frac{e^{z_i}}{\Sigma e^{z_i}}$$</p></li></ul><p>$$\frac{dA_i}{dz_i} = \frac{e^{z_i}(\Sigma e^{z_i})-(e^{z_i})^2}{(\Sigma e^{z_i})^2}$$</p><p>$$\frac{dA_i}{dz_i} = A_i(1-A_i)$$</p><ul><li>when i $\not =$ j:</li></ul></li></ul><p>$$\frac{dA_i}{dz_j} = \frac{0-e^{z_i}e^{z_j}}{(\Sigma e^z)^2}$$</p><p>$$\frac{dA_i}{dz_j} = -A_iA_j$$</p><ul><li><strong>Calculating $\LARGE{\frac{dz}{dw}, \frac{dz}{db}}$:</strong></li></ul><p>$$z = wx + b$$
$$\frac{dz}{dw} = x$$
$$\frac{dz}{db} = 1$$</p><ul><li><p><strong>Calculating $\LARGE\frac{dL}{dz}$:</strong>
$$\frac{dL}{dz} = \frac{dL}{dA}.\frac{dA}{dz}$$
$$\frac{dL}{dz} = - \Sigma\frac{y_i}{A_i}.\frac{dA}{dz}$$
$$\frac{dL}{dz}= -\frac{y_i}{A_i}.\frac{dA_i}{dz_i}- \Sigma\frac{y_j}{A_j}.\frac{dA_i}{dz_j}$$</p><ul><li>substituting the above da/dz values:
$$\frac{dL}{dz} = -\frac{y_i}{A_i}.{A_i}(1-{A_i})- \Sigma\frac{y_j}{A_j}.(-{A_i}{A_j})$$</li></ul></li></ul><p>$$= - {y_i} + {y_i}{A_i} + \Sigma{y_j}{A_i}$$
$$= - {y_i} + {y_i}{A_i} + {A_i}\Sigma{y_j}$$
$$= - {y_i} + A_i\Sigma y$$
$$\frac{dL}{dz}= A_i - y_i$$</p><p><strong>Calculating $\LARGE\frac{dL}{dw}$:</strong>
$$\frac{dL}{dw} = \frac{dL}{dz}.\frac{dz}{dw}$$
$$\frac{dL}{dw} = X(A_i - y_i)$$</p><p><strong>Calculating $\LARGE\frac{dL}{db}$:</strong>
$$\frac{dL}{db} = \frac{dL}{dz}.\frac{dz}{db}$$
$$\frac{dL}{db} = A_i - y_i$$</p><h2 id=code>Code<a hidden class=anchor aria-hidden=true href=#code>#</a></h2><pre tabindex=0><code>import numpy as np

def softmax(z):
    a = np.exp(z)
    a = a/np.sum(a, axis = 0, keepdims=True)
    return a

def init_parameters():
    w = np.random.randn(1, 5)*10
    b = np.zeros(5)
    return w, b

def cross_entropy_loss(A, y):
    loss = -np.mean(y*np.log(A))
    return loss

def forward(w, X, b):
    z = np.dot(X, w.T) + b
    A = softmax(z)
    return A

def backprop(A, X, y, w, b, alpha):
    dz = A - y
    print(&#39;dz: &#39;, dz)
    print(X)
    dw = dz*X
    db = A - y
    print(&#39;dw: &#39;, dw)
    
    w = w - alpha*dw
    b = b - alpha*db
    
    return w, b
</code></pre><pre tabindex=0><code>alpha = 0.7
X = np.array([1, 2, 3, 4, 5])
y = np.array([0, 0, 0, 0, 1])

w, b = init_parameters()
print(&#39;initial w, b: &#39;, w, b)
A = forward(w, X, b)
print(&#39;A: &#39;, A)
w, b = backprop(A, X, y, w, b, alpha)
print(&#39;after backprop: &#39;, w, b)
</code></pre></div><footer class=post-footer><ul class=post-tags><li><a href=https://srujana2505.github.io/tags/softmax/>softmax</a></li><li><a href=https://srujana2505.github.io/tags/backprop/>backprop</a></li><li><a href=https://srujana2505.github.io/tags/code/>code</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://srujana2505.github.io/>Sicilian</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>